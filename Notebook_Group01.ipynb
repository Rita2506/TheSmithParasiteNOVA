{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "2. [Baseline Model](#Baseline-Model)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "4. [Feature Selection](#Feature-Selection)\n",
    "5. [Model Selection](#Model-Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with filename:data as key:value pairs\n",
    "data = {dataset_name.replace('.xlsx', ''): pd.read_excel(f'Data/{dataset_name}') for dataset_name in os.listdir('Data') if 'xlsx' in dataset_name}\n",
    "\n",
    "# merge train datasets on PatientID\n",
    "data_frames = [data['train_demo'], data['train_habits'], data['train_health']]\n",
    "df = functools.reduce(lambda  left,right: pd.merge(left,right,on=['PatientID'], how='outer'), data_frames).set_index('PatientID')\n",
    "\n",
    "# merge test datasets on PatientID\n",
    "data_frames = [data['test_demo'], data['test_habits'], data['test_health']]\n",
    "df_test = functools.reduce(lambda  left,right: pd.merge(left,right,on=['PatientID'], how='outer'), data_frames).set_index('PatientID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look on the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values and empty strings\n",
    "pd.concat([df.isnull().sum(),df.eq('').sum()],keys=['Nulls','Empty Strings'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicated rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "# plot pairwise relationships and densities\n",
    "sns.pairplot(df, hue = 'Disease', markers = ['o', 's'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numeric features (exclude target array)\n",
    "df_numeric_features = list(df.select_dtypes(include = np.number).columns)\n",
    "df_numeric_features.remove('Disease')\n",
    "\n",
    "# boxplot of numeric features\n",
    "fig, ax = plt.subplots(math.ceil(len(df_numeric_features)/4),4, figsize = (15,10))\n",
    "for ax, feat in zip(ax.flatten(), df_numeric_features):\n",
    "    ax.boxplot(df[feat], notch = True, patch_artist = True)\n",
    "    ax.set_title(feat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical features (exclude name column)\n",
    "df_categorical_features = list(df.select_dtypes(exclude = np.number).columns)\n",
    "df_categorical_features.remove('Name')\n",
    "\n",
    "# stacked barplot of categorical features (with regard to target value)\n",
    "fig, ax = plt.subplots(len(df_categorical_features), figsize = (10,50))\n",
    "for ax, feat in zip(ax.flatten(), df_categorical_features):\n",
    "    pivot_tbl = df[[feat, 'Disease']].pivot_table(index = feat, columns = ['Disease'],  aggfunc=len)\n",
    "    graph = pivot_tbl.plot(kind='barh', stacked=True, title=feat, ax=ax)\n",
    "    ax.set_ylabel('')\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c, label_type='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline Model\n",
    "\n",
    "- LogisticRegression with only numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to data:target\n",
    "x, y = df.drop(columns = ['Disease']), df['Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numeric features\n",
    "initial_xnumeric = x.select_dtypes(include = np.number)\n",
    "initial_xnumeric_cols = initial_xnumeric.columns\n",
    "\n",
    "# stratified train-test split\n",
    "xtrain, xval, ytrain, yval = train_test_split(initial_xnumeric, y, random_state = 0 ,test_size = 0.2, shuffle = True , stratify = y)\n",
    "\n",
    "# save train split indices to track performance on the same validation set during the project\n",
    "train_indices = xtrain.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 500, random_state = 1)\n",
    "model.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = model.predict(xval)\n",
    "\n",
    "baseline_f1 = f1_score(yval, ypred)\n",
    "\n",
    "print(f'F1 score: {baseline_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "\n",
    "    # fix Birth_Year errors. here we assume that the outliers are typos\n",
    "    # the numbers 8 & 9 are pretty close to each other on a keyboard. therefore we add 100 to every year that's smaller than 1900\n",
    "    # (e.g. 1869 --> 1969)\n",
    "    dataframe['Birth_Year'] = [i + 100 if i < 1900 else i for i in dataframe['Birth_Year']]\n",
    "\n",
    "    # add Age column\n",
    "    dataframe['Age'] = [2022 - i for i in dataframe['Birth_Year']]\n",
    "\n",
    "    # add Gender column (1: Male, 0: Female)\n",
    "    # afterwards drop column Name\n",
    "    dataframe['Gender']  = [1 if i.split(' ')[0] == 'Mr.' else 0 for i in dataframe['Name']]\n",
    "    dataframe.drop(columns = ['Name'], inplace = True)\n",
    "\n",
    "    # add BMI columns\n",
    "    dataframe['BMI'] = [i/(j/100)**2 for i, j in zip(dataframe['Weight'], dataframe['Height'])]\n",
    "    dataframe['BMI_Cateogry'] = [0 if i < 18.5 else 1 if i < 25 else 2 if i < 30 else 3 if i < 35 else 4 for i in dataframe['BMI']]\n",
    "\n",
    "    # set all region values to lowercase\n",
    "    dataframe['Region'] = [i.lower() for i in dataframe['Region']]\n",
    "\n",
    "    # handle High_Cholesterol, Blood_Pressure, Physical_Health\n",
    "    # flooring and capping outliers\n",
    "    for col in ['High_Cholesterol', 'Blood_Pressure', 'Physical_Health']:\n",
    "        q25, q75, iqr = dataframe[col].quantile(.25), dataframe[col].quantile(.75), dataframe[col].quantile(.75) - dataframe[col].quantile(.25)\n",
    "        upper_lim = q75 + 1.5 * iqr\n",
    "        lower_lim = q25 - 1.5 * iqr\n",
    "        dataframe[col] = [upper_lim if i > upper_lim else lower_lim if i < lower_lim else i for i in dataframe[col]]\n",
    "\n",
    "    # encode Smoking_Habit & Exercise to binary (1: Yes, 0: No)\n",
    "    dataframe['Smoking_Habit'] = [1 if i == 'Yes' else 0 for i in dataframe['Smoking_Habit']]\n",
    "    dataframe['Exercise'] = [1 if i == 'Yes' else 0 for i in dataframe['Exercise']]    \n",
    "    \n",
    "    # manual encoding of specific feature to don't mess up the ranking\n",
    "    # all of these features have a specific ranking structure\n",
    "    # another advantage is the avoidance of dimensionality increase through One-hot-encoding all categorical features\n",
    "\n",
    "    # impute missing values in column \"Education\" with mode\n",
    "    # encode Education\n",
    "    dataframe['Education'].fillna(dataframe['Education'].mode()[0], inplace=True)\n",
    "    edu_map = {\n",
    "            'I never attended school / Other'               : 0,\n",
    "            'Elementary School (1st to 9th grade)'          : 1,\n",
    "            'High School Incomplete (10th to 11th grade)'   : 2,\n",
    "            'High School Graduate'                          : 3,\n",
    "            'University Incomplete (1 to 2 years)'          : 4,\n",
    "            'University Complete (3 or more years)'         : 5\n",
    "            }\n",
    "    dataframe['Education'] = [edu_map[i] if i in edu_map.keys() else np.nan for i in dataframe['Education']]\n",
    "\n",
    "    drink_map = {\n",
    "            'I do not consume any type of alcohol'          : 0,\n",
    "            'I consider myself a social drinker'            : 1,\n",
    "            'I usually consume alcohol every day'           : 2\n",
    "            }\n",
    "    dataframe['Drinking_Habit'] = [drink_map[i] if i in drink_map.keys() else np.nan for i in dataframe['Drinking_Habit']]\n",
    "\n",
    "    fruit_map = {\n",
    "            'Less than 1. I do not consume fruits every day.'   : 0,\n",
    "            '1 to 2 pieces of fruit in average'                 : 1,\n",
    "            '3 to 4 pieces of fruit in average'                 : 2,\n",
    "            '5 to 6 pieces of fruit in average'                 : 3,\n",
    "            'More than six pieces of fruit'                     : 4\n",
    "            }\n",
    "    dataframe['Fruit_Habit'] = [fruit_map[i] if i in fruit_map.keys() else np.nan for i in dataframe['Fruit_Habit']]\n",
    "\n",
    "    water_map = {\n",
    "            'Less than half a liter'                            : 0,\n",
    "            'More than half a liter but less than one liter'    : 1,\n",
    "            'Between one liter and two liters'                  : 2\n",
    "            }\n",
    "    dataframe['Water_Habit'] = [water_map[i] if i in water_map.keys() else np.nan for i in dataframe['Water_Habit']]\n",
    "\n",
    "    checkup_map = {\n",
    "            'Not sure'                                          : 0,\n",
    "            'More than 3 years'                                 : 1,\n",
    "            'Less than 3 years but more than 1 year'            : 2,\n",
    "            'Less than three months'                            : 3\n",
    "            }\n",
    "    dataframe['Checkup'] = [checkup_map[i] if i in checkup_map.keys() else np.nan for i in dataframe['Checkup']]\n",
    "\n",
    "    diabetes_map = {\n",
    "            'I do have diabetes'                                                            : 0,\n",
    "            'I have/had pregnancy diabetes or borderline diabetes'                          : 1,\n",
    "            \"I don't have diabetes, but I have direct family members who have diabetes.\"    : 2,\n",
    "            'Neither I nor my immediate family have diabetes.'                              : 3\n",
    "            }\n",
    "    dataframe['Diabetes'] = [diabetes_map[i] if i in diabetes_map.keys() else np.nan for i in dataframe['Diabetes']]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_preprocessed = preprocessing(x.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Check for scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), MinMaxScaler(), MinMaxScaler(feature_range = (-1,1)), RobustScaler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scaler_encoder(X,Y,scaler_options,classifier):\n",
    "\n",
    "        ranking = {'scaler':[],'score':[]}\n",
    "\n",
    "        for scl in scaler_options:\n",
    "\n",
    "                # create K-fold crossvalidation\n",
    "                skf = StratifiedKFold(shuffle=True, random_state = 0)\n",
    "\n",
    "                # scores list for every k-fold f1-score\n",
    "\n",
    "                scores = []\n",
    "                # calculate f1-score for each train-test split with the given scaler:encoder combination\n",
    "                for train_index, val_index in skf.split(X, Y):\n",
    "                        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                        y_train, y_val = Y.iloc[train_index], Y.iloc[val_index]\n",
    "\n",
    "                        numeric_features, categorical_features = X_train.select_dtypes(include = np.number).columns, X_train.select_dtypes(exclude = np.number).columns\n",
    "\n",
    "                        numeric_transformer = Pipeline(steps=[\n",
    "                                ('scaler', scl)\n",
    "                                ])\n",
    "\n",
    "                        categorical_transformer = Pipeline(steps=[\n",
    "                                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                ])\n",
    "\n",
    "                        preprocessor = ColumnTransformer(transformers=[\n",
    "                                ('cat', categorical_transformer, categorical_features),\n",
    "                                ('num', numeric_transformer, numeric_features)\n",
    "                                ])\n",
    "\n",
    "                        model = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"classifier\", classifier)\n",
    "                                ])\n",
    "                        \n",
    "                        model.fit(X_train, y_train)\n",
    "                        scores.append(round(f1_score(y_val, model.predict(X_val)),4))\n",
    "\n",
    "                ranking['scaler'].append(scl)\n",
    "                ranking['score'].append(np.mean(scores))\n",
    "\n",
    "        return pd.DataFrame(ranking).sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler_encoder(x_preprocessed, y, scalers, LogisticRegression(max_iter=500, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler_encoder(x_preprocessed, y, scalers, SVC(random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler_encoder(x_preprocessed, y, scalers, MLPClassifier(hidden_layer_sizes=(10,2), max_iter=1500, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler_encoder(x_preprocessed, y, scalers, DecisionTreeClassifier(random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaler_encoder(x_preprocessed, y, scalers, RandomForestClassifier(random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval = x_preprocessed.loc[train_indices], x_preprocessed.loc[~x_preprocessed.index.isin(train_indices)]\n",
    "ytrain, yval = y.loc[train_indices], y.loc[~x_preprocessed.index.isin(train_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prepare Train/Test Split (One-Hot-Encode, Scale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_scale(dataframe, reset_fit=False):\n",
    "\n",
    "    '''\n",
    "    One-hot-encode and scales dataframe\n",
    "    If reset_fit set to TRUE encoder & scaler get refitted\n",
    "    '''\n",
    "\n",
    "    # fit_transform if no scaler and one-hot-encoder is already fitted\n",
    "    if not('ohe' in globals() and 'scl' in globals()) or reset_fit == True:\n",
    "        xnumeric, xcategorical = dataframe.select_dtypes(include = np.number), dataframe.select_dtypes(exclude = np.number)\n",
    "        # One-hot-encoding\n",
    "        # set encoder to global variable to re-use it afterwards\n",
    "        global ohe\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        categorical_encoded = pd.DataFrame(ohe.fit_transform(xcategorical).toarray(), columns = ohe.get_feature_names_out(), index = xcategorical.index)\n",
    "        dataframe_encoded = pd.concat([xnumeric, categorical_encoded], axis = 1)\n",
    "        # Robust-scaling\n",
    "        # set scaler to global variable to re-use it afterwards\n",
    "        global scl\n",
    "        scl = RobustScaler()\n",
    "        dataframe_scaled = pd.DataFrame(scl.fit_transform(dataframe_encoded), columns = dataframe_encoded.columns, index = dataframe_encoded.index)\n",
    "        print('fit & transform successful...')\n",
    "        return dataframe_scaled\n",
    "\n",
    "    # transform only if scaler and one-hot-encoder is already fitted\n",
    "    else:\n",
    "        xnumeric, xcategorical = dataframe.select_dtypes(include = np.number), dataframe.select_dtypes(exclude = np.number)\n",
    "        # One-hot-encoding\n",
    "        categorical_encoded = pd.DataFrame(ohe.transform(xcategorical).toarray(), columns = ohe.get_feature_names_out(), index = xcategorical.index)\n",
    "        dataframe_encoded = pd.concat([xnumeric, categorical_encoded], axis = 1)\n",
    "        #Robust-scaling\n",
    "        dataframe_scaled = pd.DataFrame(scl.transform(dataframe_encoded), columns = dataframe_encoded.columns, index = dataframe_encoded.index)\n",
    "\n",
    "        print('transform successful...')\n",
    "        return dataframe_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_prepro = encode_scale(xtrain, reset_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_prepro = encode_scale(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Selection - Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Reds, fmt='.2')\n",
    "    plt.show()\n",
    "\n",
    "cor_spearman = cor_heatmap(xtrain_prepro[initial_xnumeric_cols].corr(\"spearman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestIndependence(X,Y,var,alpha=0.05):        \n",
    "    dfObserved = pd.crosstab(Y,X) \n",
    "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
    "    dfExpected = pd.DataFrame(expected, columns=dfObserved.columns, index = dfObserved.index)\n",
    "    if p<alpha:\n",
    "        result= f\"{var} is IMPORTANT for Prediction\"\n",
    "    else:\n",
    "        result= f\"{var} is NOT an important predictor. (Discard {var} from model)\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_xcategorical_col = [i for i in x.columns if i not in initial_xnumeric_cols]\n",
    "initial_xcategorical_col.remove('Name')\n",
    "initial_xcategorical_col.remove('Region')\n",
    "\n",
    "for var in initial_xcategorical_col:\n",
    "    TestIndependence(xtrain_prepro[var],ytrain, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection - Wrapper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 1)\n",
    "rfe = RFECV(model, cv = 5, scoring = 'f1', n_jobs = -1)\n",
    "rfe.fit(xtrain_prepro, ytrain)\n",
    "opt_features = list(rfe.get_feature_names_out(input_features = list(xtrain_prepro.columns)))\n",
    "print(f'Best features: {[i for i in opt_features]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_rfe = rfe.transform(xtrain_prepro)\n",
    "xval_rfe = rfe.transform(xval_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(classifiers, xtrain, ytrain, xval, yval):\n",
    "    '''\n",
    "    Plots ROC/AUC\n",
    "    classifiers input --> {'Logistic Regression': LogisticRegression(),...}\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        RocCurveDisplay.from_estimator(clf, xval, yval, ax=ax, name=name)\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC)')\n",
    "    ax.plot([0,1], [0,1], linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "classifiers = {\n",
    "                'Logistic Regression': LogisticRegression(),\n",
    "                'Support Vector Machine': SVC(),\n",
    "                'Neural Network': MLPClassifier(max_iter = 1000),\n",
    "                'Decision Tree': DecisionTreeClassifier(),\n",
    "                'Random Forest': RandomForestClassifier()\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc(classifiers, xtrain_rfe, ytrain, xval_rfe, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the learning curve for over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves(estimator, X, Y):\n",
    "    '''\n",
    "    Plots learning curve\n",
    "    '''\n",
    "\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(estimator, X, Y, cv = 5, scoring = 'f1', train_sizes = np.arange(.05,1,.05))\n",
    "    train_mean, test_mean, train_std, test_std = np.mean(train_scores, axis=1), np.mean(validation_scores, axis=1), np.std(train_scores, axis=1), np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.plot(train_sizes, train_mean, color='salmon',  label='Training score', marker = 'o')\n",
    "    plt.plot(train_sizes, test_mean, color='olive', label='Cross-validation score', marker = 's')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "learning_curves(RandomForestClassifier(), xtrain_rfe, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly overfitting --> Hyperparameter tuning could solve this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'bootstrap': [True, False],\n",
    "                'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "            }\n",
    "\n",
    "model = RandomForestClassifier(random_state = 1)\n",
    "rand_search = RandomizedSearchCV(estimator = model, param_distributions = param_grid, scoring = 'f1', n_iter = 100, cv = 3, verbose=2, random_state=3, n_jobs = -1)\n",
    "rand_search.fit(xtrain_rfe, ytrain)\n",
    "\n",
    "print(str(rand_search.best_params_).replace('{','').replace('}','').replace(\"'\",\"\").replace(':','='))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**rand_search.best_params_, random_state = 1)\n",
    "model.fit(xtrain_rfe, ytrain)\n",
    "\n",
    "ypred = model.predict(xval_rfe)\n",
    "\n",
    "print(f'F1 score: {f1_score(yval, ypred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Submission \n",
    "\n",
    "**1** - Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply encoding & scaling\n",
    "# reset_fit set to True for new fit_transform (including all train observations)\n",
    "xtrain_prepro_full = encode_scale(preprocessing(x), reset_fit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature selection on full train dataset\n",
    "xtrain_rfe_full = rfe.transform(xtrain_prepro_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with best hyperparameters\n",
    "model = RandomForestClassifier(**rand_search.best_params_, random_state = 1)\n",
    "model.fit(xtrain_rfe_full, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** - Prepare test dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing function and encode_scale function to test dataset\n",
    "xtest_prepro = encode_scale(preprocessing(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_rfe = rfe.transform(xtest_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(xtest_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.concat([pd.Series(df_test.index),pd.Series(ypred)], axis = 1)\n",
    "df_submission.rename(columns = {0:'Disease'}, inplace = True)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('Group01_Final.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8540791a84c9dd273039c82e4c4906b2f22bd80b7ff15c37ffcc5926b6e4ab9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
